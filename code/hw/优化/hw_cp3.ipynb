{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．用最速下降法求解\n",
    "$$\\min f(X)=x_1^2+25x_2^2,X_0=[2,2]^T,\\varepsilon=0.01.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution: [0 0]\n",
      "Function value at solution: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function and its gradient\n",
    "def f(X):\n",
    "    return X[0]**2 + 25*X[1]**2\n",
    "\n",
    "def grad_f(X):\n",
    "    return np.array([2*X[0], 50*X[1]])\n",
    "\n",
    "# Steepest descent method\n",
    "def steepest_descent(X0, epsilon):\n",
    "    X = X0\n",
    "    grad = grad_f(X)\n",
    "    while np.linalg.norm(grad) >= epsilon:\n",
    "        alpha = -(4 * X[0] + 2500 * X[1])/(8 * X[0] + 125000 * X[1]) # 直线搜索ls()的最佳alpha值,通过对alpha求导得到\n",
    "        X[0] = X[0] + grad[0] * alpha\n",
    "        X[1] = X[1] + grad[1] * alpha\n",
    "        grad = grad_f(X)\n",
    "    return X, f(X)\n",
    "\n",
    "# Initial point and parameters\n",
    "X0 = np.array([2, 2])\n",
    "epsilon = 0.01\n",
    "\n",
    "# Perform the optimization\n",
    "solution, value = steepest_descent(X0, epsilon)\n",
    "print(\"Solution:\", solution)\n",
    "print(\"Function value at solution:\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2．用Newton法求解\n",
    "$$\\min f(X)=60-10x_1-4x_2+x_1^2+x_2^2-x_1x_2$$\n",
    "$初始点 X_0=[0,0]^T,\\varepsilon=0.01.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (Newton's method): [8. 6.]\n",
      "Function value at solution (Newton's method): 8.0\n"
     ]
    }
   ],
   "source": [
    "# Define the function, its gradient, and Hessian\n",
    "def f_newton(X):\n",
    "    return 60 - 10*X[0] - 4*X[1] + X[0]**2 + X[1]**2 - X[0]*X[1]\n",
    "\n",
    "def grad_f_newton(X):\n",
    "    return np.array([-10 + 2*X[0] - X[1], -4 + 2*X[1] - X[0]])\n",
    "\n",
    "def hessian_f_newton(X):\n",
    "    return np.array([[2, -1], [-1, 2]])\n",
    "\n",
    "# Newton's method\n",
    "def newton_method(X0, epsilon):\n",
    "    X = X0\n",
    "    grad = grad_f_newton(X)\n",
    "    while np.linalg.norm(grad) >= epsilon:\n",
    "        P = np.linalg.solve(hessian_f_newton(X), -grad)\n",
    "        X = X + P\n",
    "        grad = grad_f_newton(X)\n",
    "    return X, f_newton(X)\n",
    "\n",
    "# Initial point and parameters\n",
    "X0 = np.array([0,0])\n",
    "epsilon = 0.01\n",
    "\n",
    "# Perform the optimization using Newton's method\n",
    "solution_newton, value_newton = newton_method(X0, epsilon)\n",
    "print(\"Solution (Newton's method):\", solution_newton)\n",
    "print(\"Function value at solution (Newton's method):\", value_newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 用修正Newton法求解\n",
    "$$\\min f(X)=4(x_1+1)^2+2(x_2-1)^2+x_1+x_2+10$$\n",
    "$初始点 X_0=[0,0]^T,\\varepsilon=0.01.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (Modified Newton's method): [-1.12618486  0.75078991]\n",
      "Function value at solution (Modified Newton's method): 9.8125068635128\n"
     ]
    }
   ],
   "source": [
    "# Define the function, its gradient, and Hessian for the modified Newton method\n",
    "def f_modified_newton(X):\n",
    "    return 4*(X[0]+1)**2 + 2*(X[1]-1)**2 + X[0] + X[1] + 10\n",
    "\n",
    "def grad_f_modified_newton(X):\n",
    "    return np.array([8*(X[0]+1) + 1, 4*(X[1]-1) + 1])\n",
    "\n",
    "def hessian_f_modified_newton(X):\n",
    "    return np.array([[8, 0], [0, 4]])\n",
    "\n",
    "# Modified Newton's method\n",
    "def modified_newton_method(X0, epsilon):\n",
    "    X = X0\n",
    "    grad = grad_f_modified_newton(X)\n",
    "    while np.linalg.norm(grad) >= epsilon:\n",
    "        P = np.linalg.solve(hessian_f_modified_newton(X), -grad)\n",
    "        t = (8* X[0] + 4*X[1] + 6)/( 9*X[0] + 5*X[1] + 35/8) # 直线搜索ls()的最佳t值,通过对t求导得到\n",
    "        X = X + t * P\n",
    "        grad = grad_f_modified_newton(X)\n",
    "    return X, f_modified_newton(X)\n",
    "\n",
    "# Initial point and parameters\n",
    "X0 = np.array([0, 0])\n",
    "epsilon = 0.01\n",
    "\n",
    "# Perform the optimization using the modified Newton method\n",
    "solution_modified_newton, value_modified_newton = modified_newton_method(X0, epsilon)\n",
    "print(\"Solution (Modified Newton's method):\", solution_modified_newton)\n",
    "print(\"Function value at solution (Modified Newton's method):\", value_modified_newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4．用共轭梯度法求解\n",
    "$$\\min(x_1^{2}+4x_2^{2})$$\n",
    "$初始点 X_0=[1,1]^T,\\varepsilon=0.01.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (Conjugate Gradient method): [ 1.00365696e-03 -6.27285599e-05]\n",
      "Function value at solution (Conjugate Gradient method): 1.0230667798482407e-06\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "# Define the function and its gradient\n",
    "def f_conjugate_gradient(X):\n",
    "    return X[0]**2 + 4*X[1]**2\n",
    "\n",
    "def grad_f_conjugate_gradient(X):\n",
    "    return np.array([2*X[0], 8*X[1]])\n",
    "\n",
    "# Conjugate gradient method\n",
    "def conjugate_gradient_method(X0, epsilon):\n",
    "    iter = 0\n",
    "    X = X0\n",
    "    r = -1 * grad_f_conjugate_gradient(X)\n",
    "    d = r\n",
    "    n = 2\n",
    "    while np.linalg.norm(grad_f_conjugate_gradient(X)) >= epsilon:\n",
    "        ### 先找区间：\n",
    "        #lower,upper = find_interval(X,d)\n",
    "        # ## 进行一维搜索\n",
    "        # t = duifen_sousuo(X, d,lower,upper)\n",
    "        def f_of_t(t):\n",
    "            return f_conjugate_gradient(X + t*d)\n",
    "        def grad_f_of_t(t):\n",
    "            return np.dot(grad_f_conjugate_gradient(X + t*d), d)\n",
    "        t = scipy.optimize.line_search(f_of_t, grad_f_of_t, 0, 1)[0]\n",
    "        X_new = X + t*d\n",
    "        if iter + 1 == n:   \n",
    "            iter = 0\n",
    "            r = -grad_f_conjugate_gradient(X)\n",
    "            d = r\n",
    "            n = 2\n",
    "        else:\n",
    "            lambda_k = np.linalg.norm(grad_f_conjugate_gradient(X_new)) / np.linalg.norm(grad_f_conjugate_gradient(X))\n",
    "            d = -grad_f_conjugate_gradient(X_new) + lambda_k * d\n",
    "            X = X_new\n",
    "            iter += 1\n",
    "    return X, f_conjugate_gradient(X)\n",
    "\n",
    "# Initial point and parameters\n",
    "X0 = np.array([1., 1.])\n",
    "epsilon = 0.01\n",
    "\n",
    "# Perform the optimization using the conjugate gradient method\n",
    "solution_conjugate_gradient, value_conjugate_gradient = conjugate_gradient_method(X0, epsilon)\n",
    "print(\"Solution (Conjugate Gradient method):\", solution_conjugate_gradient)\n",
    "print(\"Function value at solution (Conjugate Gradient method):\", value_conjugate_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5．用共轭梯度法求解 \n",
    "$$\\min(2x_1^{2}+x_2^{2}-x_1x_2)$$\n",
    "自定初始点$\\varepsilon=0.01.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (Conjugate Gradient method): [0.00228977 0.00228977]\n",
      "Function value at solution (Conjugate Gradient method): 1.0486111932550557e-05\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "# Define the function and its gradient\n",
    "def f_conjugate_gradient(X):\n",
    "    return 2* X[0]**2 + X[1]**2 - X[0]*X[1]\n",
    "\n",
    "def grad_f_conjugate_gradient(X):\n",
    "    return np.array([4*X[0]-X[1], 2*X[1]-X[0]])\n",
    "\n",
    "# Conjugate gradient method\n",
    "def conjugate_gradient_method(X0, epsilon):\n",
    "    iter = 0\n",
    "    X = X0\n",
    "    r = -1 * grad_f_conjugate_gradient(X)\n",
    "    d = r\n",
    "    n = 2\n",
    "    while np.linalg.norm(grad_f_conjugate_gradient(X)) >= epsilon:\n",
    "        ### 先找区间：\n",
    "        #lower,upper = find_interval(X,d)\n",
    "        # ## 进行一维搜索\n",
    "        # t = duifen_sousuo(X, d,lower,upper)\n",
    "        def f_of_t(t):\n",
    "            return f_conjugate_gradient(X + t*d)\n",
    "        def grad_f_of_t(t):\n",
    "            return np.dot(grad_f_conjugate_gradient(X + t*d), d)\n",
    "        t = scipy.optimize.line_search(f_of_t, grad_f_of_t, 0, 1)[0]\n",
    "        X_new = X + t*d\n",
    "        if iter + 1 == n:   \n",
    "            iter = 0\n",
    "            r = -grad_f_conjugate_gradient(X)\n",
    "            d = r\n",
    "            n = 2\n",
    "        else:\n",
    "            lambda_k = np.linalg.norm(grad_f_conjugate_gradient(X_new)) / np.linalg.norm(grad_f_conjugate_gradient(X))\n",
    "            d = -grad_f_conjugate_gradient(X_new) + lambda_k * d\n",
    "            X = X_new\n",
    "            iter += 1\n",
    "    return X, f_conjugate_gradient(X)\n",
    "\n",
    "# Initial point and parameters\n",
    "X0 = np.array([1., 1.])\n",
    "epsilon = 0.01\n",
    "\n",
    "# Perform the optimization using the conjugate gradient method\n",
    "solution_conjugate_gradient, value_conjugate_gradient = conjugate_gradient_method(X0, epsilon)\n",
    "print(\"Solution (Conjugate Gradient method):\", solution_conjugate_gradient)\n",
    "print(\"Function value at solution (Conjugate Gradient method):\", value_conjugate_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6．用DFP法求解\n",
    "$$\\min f(X) = 4(x_1-5)^{2}+(x_2-6)^{2}$$\n",
    "$初始点 X_0=[8,9]^T,\\varepsilon=0.01.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (DFP method): [5. 6.]\n",
      "Function value at solution (DFP method): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the function, its gradient\n",
    "def f_dfp(X):\n",
    "    return 4*(X[0]-5)**2 + (X[1]-6)**2\n",
    "\n",
    "def grad_f_dfp(X):\n",
    "    return np.array([8*(X[0]-5), 2*(X[1]-6)])\n",
    "\n",
    "# DFP method\n",
    "def dfp_method(X0, epsilon):\n",
    "    X = X0\n",
    "    n = len(X0)\n",
    "    H = np.eye(n)  # Initial Hessian approximation\n",
    "    grad = grad_f_dfp(X)\n",
    "    while np.linalg.norm(grad) >= epsilon:\n",
    "        P = -np.dot(H, grad)\n",
    "        alpha = scipy.optimize.line_search(f_dfp, grad_f_dfp, X, P)[0]\n",
    "        X_new = X + alpha * P\n",
    "        grad_new = grad_f_dfp(X_new)\n",
    "        delta_X = X_new - X\n",
    "        delta_grad = grad_new - grad\n",
    "        H = H + np.outer(delta_X, delta_X) / np.dot(delta_X, delta_grad) - np.dot(H, np.outer(delta_grad, delta_grad)).dot(H) / np.dot(delta_grad, H.dot(delta_grad))\n",
    "        X = X_new\n",
    "        grad = grad_new\n",
    "    return X, f_dfp(X)\n",
    "\n",
    "# Initial point and parameters\n",
    "X0 = np.array([8, 9])\n",
    "epsilon = 0.01\n",
    "\n",
    "# Perform the optimization using the DFP method\n",
    "solution_dfp, value_dfp = dfp_method(X0, epsilon)\n",
    "print(\"Solution (DFP method):\", solution_dfp)\n",
    "print(\"Function value at solution (DFP method):\", value_dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 用坐标轮换法求解\n",
    "$$\\min f(X) = x_1^{2}+x_2^{2}- x_1x_2 - 10 x_1 - 4x_2 + 60$$\n",
    "$初始点 X_0=[0,0]^T,\\varepsilon=0.1.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (Coordinate Rotation method): [7 5]\n",
      "Function value at solution (Coordinate Rotation method): 9\n"
     ]
    }
   ],
   "source": [
    "# Define the function and its partial derivatives\n",
    "def f_coordinate_rotation(X):\n",
    "    return X[0]**2 + X[1]**2 - X[0]*X[1] - 10*X[0] - 4*X[1] + 60\n",
    "\n",
    "def partial_derivative_x1(X):\n",
    "    return 2*X[0] - X[1] - 10\n",
    "\n",
    "def partial_derivative_x2(X):\n",
    "    return 2*X[1] - X[0] - 4\n",
    "\n",
    "# Coordinate rotation method\n",
    "def coordinate_rotation_method(X0, epsilon):\n",
    "    X = X0.copy()\n",
    "    while True:\n",
    "        X_old = X.copy()\n",
    "        \n",
    "        # Update x1 while keeping x2 constant\n",
    "        X[0] = (10 + X[1]) / 2\n",
    "        \n",
    "        # Update x2 while keeping x1 constant\n",
    "        X[1] = (4 + X[0]) / 2\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(X - X_old) < epsilon:\n",
    "            break\n",
    "            \n",
    "    return X, f_coordinate_rotation(X)\n",
    "\n",
    "# Initial point and parameters\n",
    "X0 = np.array([0, 0])\n",
    "epsilon = 0.1\n",
    "\n",
    "# Perform the optimization using the coordinate rotation method\n",
    "solution_coordinate_rotation, value_coordinate_rotation = coordinate_rotation_method(X0, epsilon)\n",
    "print(\"Solution (Coordinate Rotation method):\", solution_coordinate_rotation)\n",
    "print(\"Function value at solution (Coordinate Rotation method):\", value_coordinate_rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8．用单纯形法求解 \n",
    "$$ \\min f(X) = x_1^{2}+x_2^{2} - 4x_1 - 8x_2 + 5 $$\n",
    "给定初始单纯形顶点为:\n",
    "$X_1=[0,0]^T,X_2=[0.965,0.259]^T,X_2=[0.259,0.965]^T,\\varepsilon=0.1,\\alpha = 1.1,\\beta = 1, \\gamma = 0.5.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "终止，迭代次数为9\n",
      "极值点为\n",
      "1.9542159471874994,4.136377006562501\n",
      "函数最小值为-14.9793\n",
      "Solution (simplex method): [[1.95421595]\n",
      " [4.13637701]]\n",
      "Function value at solution (simplex method): [-14.97930513]\n"
     ]
    }
   ],
   "source": [
    "# Define the function\n",
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2 - 4*x[0] - 8*x[1] +5\n",
    "\n",
    "\n",
    "# Define simplex method\n",
    "def simplex(f,x1,x2,x3,epsilon):\n",
    "    k=0\n",
    "    ## 排序，使得x3最小，x1最大：\n",
    "    x1,x2,x3 = sorted([x1,x2,x3],key=f,reverse=True)\n",
    "    while 1:\n",
    "        x4 = (x2 + x3) / 2\n",
    "        d = x4 - x1\n",
    "        x5 = x4 + d\n",
    "        if f(x5) < f(x3):\n",
    "            x6 = x4 + alpha * d\n",
    "            if f(x6) < f(x5):\n",
    "                triangle = [x2, x3, x6]\n",
    "            else:\n",
    "                triangle = [x2, x3, x5]\n",
    "        elif f(x5) < f(x2):\n",
    "            triangle = [x2, x5, x3]\n",
    "        elif f(x5) < f(x1):\n",
    "            x7 = x4 + gama * d\n",
    "            triangle = [x7, x2, x3]\n",
    "        else:\n",
    "            x8 = x4 - gama * d\n",
    "            if f(x8) < f(x1):\n",
    "                triangle = [x8, x2, x3]\n",
    "            else:\n",
    "                x9 = (x1 + x2) / 2\n",
    "                x10 = (x1 + x3) / 2\n",
    "                triangle = [x9, x10, x3]\n",
    "        triangle[0],triangle[1],triangle[2] = sorted([triangle[0],triangle[1],triangle[2]],key=f,reverse=True)\n",
    "        x1 = triangle[0]\n",
    "        x2 = triangle[1]\n",
    "        x3 = triangle[2]\n",
    "        k=k+1\n",
    "        if (f(x1) - f(x3))**2 + (f(x2) - f(x3))**2 <= epsilon:\n",
    "            X0 = (x1+x2+x3)/3\n",
    "            print(f'终止，迭代次数为{k}\\n极值点为\\n{X0[0][0]:},{X0[1][0]:}\\n函数最小值为{f(X0)[0]:.4f}')\n",
    "            return X0,f(X0)\n",
    "\n",
    "# Initial point and parameters\n",
    "x1 = np.array([[0],[0]])\n",
    "x2 = np.array([[0.965],[0.259]])\n",
    "x3 = np.array([[0.259],[0.965]])\n",
    "epsilon = 0.1\n",
    "alpha = 1.1\n",
    "beta = 1\n",
    "gama = 0.5\n",
    "\n",
    "# Perform the optimization using the coordinate rotation method\n",
    "solution_simplex, value_simplex = simplex(f,x1,x2,x3, epsilon)\n",
    "print(\"Solution (simplex method):\", solution_simplex)\n",
    "print(\"Function value at solution (simplex method):\", value_simplex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
